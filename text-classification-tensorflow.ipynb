{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Classification with Tensorflow and HuggingFace transformers</h1>\n",
    "<h3>The purpose of this notebook is to fine tune a pretrained model from hugging face to classify given text string as one of the labels. In order to fine tune the model, we need a sample data of text with proper labels. This notebook is gneric, so you can feed in any data as long as it has 2 columns i.e., text and label</h3>\n",
    "\n",
    "<h2>Parameters for this notebook</h2>\n",
    "<h3>\n",
    "Model Name: bert-base-multilingual-uncased</br>\n",
    "Model Source: Hugging Face </br>\n",
    "Trainable Parameters: 1676M</br>\n",
    "</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install \\\n",
    "    tensorflow==2.13.0 \\\n",
    "    transformers==4.34.0 \\\n",
    "    datasets==2.14.5 \\\n",
    "    evaluate==0.4.1 \\\n",
    "    scikit-learn==1.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import create_optimizer\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import Dataset\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, random\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seeds\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>This notebook was executed on single T4 GPU. TF by default allocate all GPU memory while initizalizng the model. set_memory_growth is used here to allocate on demand. This is useful, when running multiple notebooks on same GPU</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Parameters for preparing data. I trained this model in multiple iterations, sometimes duplicating the data to maintain label balance.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labeled_data_path = 'sample-data.csv'\n",
    "t_day = datetime.now().strftime('%Y-%m-%d-%H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load Data</h2>\n",
    "</h3>Load the data and do some basic processing, split into test, train and validation sets and finally converted to datasets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data into dataframe\n",
    "data = pd.read_csv(labeled_data_path)\n",
    "\n",
    "# set datatype of text to string\n",
    "data['text'] = data['text'].astype(str)\n",
    "\n",
    "# if labels are strings, then use astype as str\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "print(f'No. of rows in data: {data.shape[0]}')\n",
    "\n",
    "# print first 10 rows\n",
    "print(f'First few rows looks like this:\\n{data[:10]}')\n",
    "\n",
    "# unique label values and counts\n",
    "print(f'Label counts: {data.label.value_counts()}')\n",
    "\n",
    "# split data into train, valid and test sets\n",
    "train_df, test_df = train_test_split(data, random_state=42, train_size=0.9, stratify=data.label.values)\n",
    "train_df, valid_df = train_test_split(train_df, random_state=42, train_size=0.8, stratify=train_df.label.values)\n",
    "print(f'train: {train_df.shape}; valid: {valid_df.shape}; test: {test_df.shape}')\n",
    "\n",
    "# convert to datasets.Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "print('Converted to datasets.Dataset')\n",
    "\n",
    "# print first few rows of train_dataset\n",
    "print('First few rows of train_dataset:')\n",
    "print(train_dataset[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Model hyper parameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "\n",
    "model_name = 'bert-base-multilingual-uncased'  #drop in any other model from huggingface model hub\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "label_length = len(data.label.unique())\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tokenize data</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# tokenize dataset\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], truncation=True)\n",
    "\n",
    "# tokenize train and valid datasets\n",
    "train_tokenized = train_dataset.map(tokenize, batched=True)\n",
    "valid_tokenized = valid_dataset.map(tokenize, batched=True)\n",
    "print('train and valid datasets tokenized')\n",
    "\n",
    "# print first 2 rows of tokenized train dataset\n",
    "print(train_tokenized[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Initialize Model</h2>\n",
    "Load pretrained model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# initialize model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=label_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Convert tokenized dataset to TF datasets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set format for tensorflow\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "# prepare tensorflow datasets\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    train_tokenized,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    valid_tokenized,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define optimizer and custom metric callback function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "batches_per_epoch = len(train_tokenized) / batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
    "\n",
    "# custom metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# metric callback for training and validation accuracy\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "\n",
    "callbacks = [metric_callback]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Compile the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "h = model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=num_epochs, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Plot accuracy and loss for training and valication set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get accuracy and loss from history\n",
    "acc = h.history['accuracy']\n",
    "val_acc = h.history['val_accuracy']\n",
    "loss = h.history['loss']\n",
    "val_loss = h.history['val_loss']\n",
    "\n",
    "# plot accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title(f'Training and validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot loss\n",
    "plt.clf()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title(f'Training and validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predict the test set</h2>\n",
    "Predict on test set and check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict the test data\n",
    "test_true = test_df.label.values\n",
    "test_tokenized = test_dataset.map(tokenize, batched=True)\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    test_tokenized,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_pred = model.predict(tf_test_set)\n",
    "test_pred = np.argmax(test_pred.logits, axis=1)\n",
    "test_df['pred_label'] = test_pred\n",
    "\n",
    "# get accuracy\n",
    "accuracy = accuracy_score(test_true, test_pred)\n",
    "print(f'Test Accuracy: {accuracy:.2%}')\n",
    "\n",
    "# get classification report\n",
    "report = classification_report(test_true, test_pred)\n",
    "print(report)\n",
    "\n",
    "# get confusion matrix\n",
    "matrix = confusion_matrix(test_true, test_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Save model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the model with the tokenizer\n",
    "model.save_pretrained('model/saved_model-%s'%(t_day))\n",
    "tokenizer.save_pretrained('model/saved_model-%s-%s'%(t_day))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
